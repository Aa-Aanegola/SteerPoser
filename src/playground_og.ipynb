{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/workspace/activation-steering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(EE) \n",
      "Fatal server error:\n",
      "(EE) Server is already active for display 99\n",
      "\tIf this server is no longer running, remove /tmp/.X99-lock\n",
      "\tand start again.\n",
      "(EE) \n"
     ]
    }
   ],
   "source": [
    "# Start Xvfb manually (xvfb-run does this under the hood)\n",
    "subprocess.Popen([\n",
    "    'Xvfb', ':99', '-screen', '0', '1280x1024x24', '+extension', 'GLX', '+render', '-noreset'\n",
    "])\n",
    "\n",
    "time.sleep(2)  # Give Xvfb time to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables to match your working terminal setup\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['LIBGL_ALWAYS_SOFTWARE'] = '1'\n",
    "#os.environ['LIBGL_DRIVERS_PATH'] = '/usr/lib/x86_64-linux-gnu/dri' - for google cloud\n",
    "#os.environ['LD_PRELOAD'] = '/usr/lib/x86_64-linux-gnu/libffi.so.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGL renderer string: llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "OpenGL version string: 4.5 (Compatibility Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GLX version: 1.4\n",
      "    GLX_MESA_copy_sub_buffer, GLX_MESA_query_renderer, GLX_MESA_swap_control, \n",
      "    GLX_MESA_query_renderer, GLX_OML_swap_method, GLX_SGIS_multisample, \n",
      "Extended renderer info (GLX_MESA_query_renderer):\n",
      "OpenGL renderer string: llvmpipe (LLVM 15.0.7, 256 bits)\n"
     ]
    }
   ],
   "source": [
    "!glxinfo | grep 'OpenGL renderer string'\n",
    "!glxinfo | grep 'OpenGL version string'\n",
    "!glxinfo | grep 'GLX version'\n",
    "!glxinfo | grep 'renderer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rlbench'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minterfaces\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_LMP, LMP_interface\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvisualizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValueMapVisualizer\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrlbench_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VoxPoserRLBench\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_lmp_objects\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SteerKep/SteerPoser/src/envs/rlbench_env.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mo3d\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlbench\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maction_modes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maction_mode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MoveArmThenGripper\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlbench\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maction_modes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marm_action_modes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArmActionMode, EndEffectorPoseViaPlanning\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrlbench\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maction_modes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgripper_action_modes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Discrete, GripperActionMode\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rlbench'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Key Here\"\n",
    "from arguments import get_config\n",
    "from interfaces import setup_LMP, LMP_interface\n",
    "from visualizers import ValueMapVisualizer\n",
    "from envs.rlbench_env import VoxPoserRLBench\n",
    "from utils import set_lmp_objects\n",
    "import numpy as np\n",
    "from rlbench import tasks\n",
    "from LMP import LMP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "## voxel resolution: [0.0105 0.0131 0.01  ]\n",
      "##################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = get_config('rlbench')\n",
    "# uncomment this if you'd like to change the language model (e.g., for faster speed or lower cost)\n",
    "# for lmp_name, cfg in config['lmp_config']['lmps'].items():\n",
    "#     cfg['model'] = 'gpt-3.5-turbo'\n",
    "\n",
    "# initialize env and voxposer ui\n",
    "visualizer = ValueMapVisualizer(config['visualizer'])\n",
    "env = VoxPoserRLBench(visualizer=visualizer)\n",
    "lmps, lmp_env = setup_LMP(env, config, debug=False)\n",
    "voxposer_ui = lmps['plan_ui']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "By default we use one of the instructions that come with each task. However, you may treat each task as simply a scene initialization from RLBench, and feel free to try any task you can come up with for the scene.\n",
    "\n",
    "Note:\n",
    "- Whether an instruction can be executed or not depends on 1) whether relevant objects are available, and 2) capabilities of the overall algorithm.\n",
    "- Each execution may produce one or more visualizations. You may view them in \"./visualizations/\" folder.\n",
    "- The prompts are adapted with minimal change from the real-world environment in the VoxPoser paper. If a task failure is due to incorrect code generated by the LLM, feel free to modify the relevant prompt in \"./prompts/\" folder.\n",
    "- You may view the reward by printing \"env.latest_reward\". These are computed by RLBench for each task.\n",
    "- To inspect in viewer without performing any action, you may call \"env.rlbench_env._scene.step()\" in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment this to show all available tasks in rlbench\n",
    "# # NOTE: in order to run a new task, you need to add the list of objects (and their corresponding env names) to the \"task_object_names.json\" file. See README for more details.\n",
    "# print([task for task in dir(tasks) if task[0].isupper() and not '_' in task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded task: set_the_table\n"
     ]
    }
   ],
   "source": [
    "# below are the tasks that have object names added to the \"task_object_names.json\" file\n",
    "# uncomment one to use\n",
    "# env.load_task(tasks.PutRubbishInBin)\n",
    "# env.load_task(tasks.LampOff)\n",
    "# env.load_task(tasks.OpenWineBottle)\n",
    "# env.load_task(tasks.PushButton)\n",
    "# env.load_task(tasks.TakeOffWeighingScales)\n",
    "# env.load_task(tasks.MeatOffGrill)\n",
    "# env.load_task(tasks.SlideBlockToTarget)\n",
    "# env.load_task(tasks.TakeLidOffSaucepan)\n",
    "# env.load_task(tasks.TakeUmbrellaOutOfUmbrellaStand)\n",
    "env.load_task(tasks.SetTheTable)\n",
    "\n",
    "descriptions, obs = env.reset()\n",
    "set_lmp_objects(lmps, env.get_object_names())  # set the object names to be used by voxposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"planner\" generated code\n",
      "## context: \"objects = ['chocolate', 'apple', 'granola', 'soda']\"\n",
      "########################################\n",
      "objects = [\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mapple\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgranola\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msoda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Query: Pick up a snack for me.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "composer(\u001b[33m\"\u001b[39;49;00m\u001b[33mgrasp the chocolate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "composer(\u001b[33m\"\u001b[39;49;00m\u001b[33mback to default pose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# done\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"composer\" generated code\n",
      "########################################\n",
      "\u001b[37m# Query: grasp the chocolate.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "movable = parse_query_obj(\u001b[33m'\u001b[39;49;00m\u001b[33mgripper\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "affordance_map = get_affordance_map(\u001b[33m'\u001b[39;49;00m\u001b[33ma point at the center of the chocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "gripper_map = get_gripper_map(\u001b[33m'\u001b[39;49;00m\u001b[33mopen everywhere except 1cm around the chocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "execute(movable, affordance_map=affordance_map, gripper_map=gripper_map)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"parse_query_obj\" generated code\n",
      "## context: \"objects = ['chocolate', 'apple', 'granola', 'soda']\"\n",
      "########################################\n",
      "objects = [\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mapple\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgranola\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msoda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Query: gripper.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "gripper = detect(\u001b[33m'\u001b[39;49;00m\u001b[33mgripper\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "ret_val = gripper\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"get_affordance_map\" generated code\n",
      "########################################\n",
      "\u001b[37m# Query: a point at the center of the chocolate.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "affordance_map = get_empty_affordance_map()\u001b[37m\u001b[39;49;00m\n",
      "chocolate = parse_query_obj(\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "x, y, z = chocolate.position\u001b[37m\u001b[39;49;00m\n",
      "affordance_map[x, y, z] = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ret_val = affordance_map\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OpenAI API call took 3.03s ***\n",
      "########################################\n",
      "## \"get_gripper_map\" generated code\n",
      "########################################\n",
      "\u001b[37m# Query: open everywhere except 1cm around the chocolate.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "gripper_map = get_empty_gripper_map()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# open everywhere\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "gripper_map[:, :, :] = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# close when 1cm around the chocolate\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "chocolate = parse_query_obj(\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "set_voxel_by_radius(gripper_map, chocolate.position, radius_cm=\u001b[34m1\u001b[39;49;00m, value=\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "ret_val = gripper_map\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "*** OpenAI API call took 1.62s ***\n",
      "########################################\n",
      "## \"parse_query_obj\" generated code\n",
      "## context: \"objects = ['chocolate', 'apple', 'granola', 'soda']\"\n",
      "########################################\n",
      "objects = [\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mapple\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgranola\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msoda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Query: chocolate.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "chocolate = detect(\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "ret_val = chocolate\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"parse_query_obj\" generated code\n",
      "## context: \"objects = ['chocolate', 'apple', 'granola', 'soda']\"\n",
      "########################################\n",
      "objects = [\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mapple\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mgranola\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msoda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Query: chocolate.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "chocolate = detect(\u001b[33m'\u001b[39;49;00m\u001b[33mchocolate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "ret_val = chocolate\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "[planners.py | 1:59:35.984] start\n",
      "[planners.py | 1:59:36.245] start optimizing, start_pos: [52 49 71]\n",
      "[planners.py | 1:59:36.534] optimization finished; path length: 175\n",
      "[planners.py | 1:59:36.557] after postprocessing, path length: 29\n",
      "[planners.py | 1:59:36.557] last waypoint: [33. 23.  9.]\n",
      "\u001b[94m[interfaces.py | 1:59:36] planner time: 0.577s\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:36] overwriting gripper to less common value for the last waypoint\u001b[0m\n",
      "** saving visualization to ./visualizations/1:59:36.html ...\n",
      "** saving visualization to ./visualizations/latest.html ...\n",
      "** save to ./visualizations/1:59:36.html\n",
      "\u001b[94m[interfaces.py | 1:59:36] start executing path via controller (31 waypoints)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:38] completed waypoint 1 (wp: [ 0.286 -0.01   1.469], actual: [ 0.286 -0.01   1.469], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.743)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:40] completed waypoint 2 (wp: [ 0.257 -0.032  1.449], actual: [ 0.257 -0.032  1.449], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.708)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:42] completed waypoint 3 (wp: [ 0.231 -0.057  1.429], actual: [ 0.231 -0.056  1.429], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.674)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:44] completed waypoint 4 (wp: [ 0.206 -0.083  1.409], actual: [ 0.206 -0.083  1.408], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.639)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:46] completed waypoint 5 (wp: [ 0.183 -0.111  1.388], actual: [ 0.184 -0.11   1.388], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.606)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:48] completed waypoint 6 (wp: [ 0.163 -0.139  1.368], actual: [ 0.164 -0.138  1.368], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.574)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:49] completed waypoint 7 (wp: [ 0.144 -0.167  1.348], actual: [ 0.145 -0.167  1.348], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.542)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:51] completed waypoint 8 (wp: [ 0.128 -0.196  1.328], actual: [ 0.128 -0.195  1.328], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.512)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:53] completed waypoint 9 (wp: [ 0.113 -0.223  1.308], actual: [ 0.114 -0.222  1.308], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.484)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:54] completed waypoint 10 (wp: [ 0.1   -0.25   1.287], actual: [ 0.101 -0.249  1.287], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.457)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:56] completed waypoint 11 (wp: [ 0.086 -0.284  1.257], actual: [ 0.086 -0.283  1.257], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.42)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:58] completed waypoint 12 (wp: [ 0.079 -0.303  1.237], actual: [ 0.08  -0.302  1.237], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.397)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 1:59:59] completed waypoint 13 (wp: [ 0.074 -0.319  1.217], actual: [ 0.075 -0.318  1.217], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.376)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:1] completed waypoint 14 (wp: [ 0.071 -0.332  1.196], actual: [ 0.071 -0.332  1.197], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.354)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:3] completed waypoint 15 (wp: [ 0.07  -0.342  1.176], actual: [ 0.07  -0.341  1.177], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.334)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:4] completed waypoint 16 (wp: [ 0.07  -0.349  1.156], actual: [ 0.07  -0.349  1.156], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.313)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:6] completed waypoint 17 (wp: [ 0.07  -0.354  1.136], actual: [ 0.07  -0.353  1.136], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.293)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:7] completed waypoint 18 (wp: [ 0.07  -0.357  1.116], actual: [ 0.07  -0.356  1.116], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.273)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:9] completed waypoint 19 (wp: [ 0.07  -0.358  1.095], actual: [ 0.07  -0.357  1.096], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.253)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:10] completed waypoint 20 (wp: [ 0.069 -0.358  1.075], actual: [ 0.069 -0.357  1.075], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.232)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:12] completed waypoint 21 (wp: [ 0.067 -0.357  1.054], actual: [ 0.067 -0.357  1.054], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.211)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:13] completed waypoint 22 (wp: [ 0.064 -0.355  1.031], actual: [ 0.064 -0.355  1.032], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.19)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:15] completed waypoint 23 (wp: [ 0.062 -0.355  1.01 ], actual: [ 0.061 -0.355  1.01 ], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.168)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:17] completed waypoint 24 (wp: [ 0.059 -0.357  0.989], actual: [ 0.059 -0.357  0.99 ], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.148)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:19] completed waypoint 25 (wp: [ 0.058 -0.36   0.97 ], actual: [ 0.058 -0.36   0.969], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.128)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:20] completed waypoint 26 (wp: [ 0.057 -0.36   0.952], actual: [ 0.057 -0.36   0.952], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.112)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:22] completed waypoint 27 (wp: [ 0.056 -0.362  0.936], actual: [ 0.056 -0.362  0.936], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.096)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:24] completed waypoint 28 (wp: [ 0.055 -0.364  0.923], actual: [ 0.055 -0.363  0.923], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.083)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:27] completed waypoint 29 (wp: [ 0.075 -0.351  0.843], actual: [ 0.076 -0.358  0.853], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.012)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:27] skip waypoint 30 because it is moving in opposite direction of the final target\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:28] completed waypoint 31 (wp: [ 0.075 -0.351  0.843], actual: [ 0.076 -0.358  0.853], target: [ 0.075 -0.351  0.843], start: [ 0.286 -0.01   1.469], dist2target: 0.012)\u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:29] reached target; terminating \u001b[0m\n",
      "\u001b[94m[interfaces.py | 2:0:29] finished executing path via controller\u001b[0m\n",
      "(using cache) *** OpenAI API call took 0.00s ***\n",
      "########################################\n",
      "## \"composer\" generated code\n",
      "########################################\n",
      "\u001b[37m# Query: back to default pose.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "reset_to_default_pose()\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = np.random.choice(descriptions)\n",
    "voxposer_ui(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Base)",
   "language": "python",
   "name": "base-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
